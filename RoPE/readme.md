

# Requirement
* docker engine
* nvidia gpu drivers
  * driver version 535.161.07 tested on my local env

# Env setting

* Build a docker image
  ```bash
  git clone https://github.com/zetwhite/triton-practice.git 
  cd triton-practice
  
  docker build -t nvidia_env:1.0 -f nvidia_env.dockerfile .
  ```

* Run a container
  ```bash
  cd triton-practice
  
  docker run \
    -v ./RoPE:/home/RoPE \
    --gpus all \
    --ipc=host \
    --ulimit memlock=-1 \
    --ulimit stack=67108864 \
    --rm -it nvidia_env:1.0
  ```

# Run
* After attaching the container, you could run the code in this directory. 

* `value_check.py` ëŠ” tritonìœ¼ë¡œ ì§ì ‘ êµ¬í˜„í•œ RoPE forward, backword í•¨ìˆ˜ì™€ [Transformer Engineì— ìˆëŠ” í•¨ìˆ˜](https://github.com/NVIDIA/TransformerEngine/blob/5b90b7f5ed67b373bc5f843d1ac3b7a8999df08e/transformer_engine/pytorch/attention.py#L1037-L1078)ì˜ í…ì„œê°’ì„ ë¹„êµí•©ë‹ˆë‹¤. 
    ```bash
    python3 value_check.py

    # expected log
    # Let's check accuracy, by comparing transformer_engine implementation and my own kernel :D
    #
    # >> forward
    #  - RoPE_forward() output is exactly SAME with TransformerEngine's
    #
    # >> backward
    #  - RoPE_backward() output is exactly SAME with TransformerEngine's
    # 
    ``` 

* `perf.py`ëŠ” ì§ì ‘ êµ¬í˜„í•œ RoPE forwardí•¨ìˆ˜ì™€ Transformer Engineì˜ í•¨ìˆ˜ì˜ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    * ë‹¤ë§Œ, ì—¬ê¸°ì„œ ë¹„êµë˜ëŠ” Transformer Engineì˜ RoPEí•¨ìˆ˜(`apply_rotary_pos_embed`)ëŠ” [fused](https://github.com/NVIDIA/TransformerEngine/commit/6c1a8bb5dffbce386380f8e5a12c45f7032d9b76#diff-8215778f23231390f7e41e1339eed64843646d7aba265b8dbf3d68a76c1a647f)ê°€ ì ìš©ë˜ê¸°ì „ì˜ í•¨ìˆ˜ ì…ë‹ˆë‹¤. <sub> ë¡œì»¬ì—ì„œ ìµœì‹ ë²„ì „ì˜ Transformer Engineì„ ë¹Œë“œí•˜ëŠ”ë°, ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ ğŸ˜¢ </sub>  
    ``` bash 
    python perf.py
    # you could check a simple result image(*.png) in perf
    ```


# Note 
forwardì™€ backwardì— ëŒ€í•œ ê°„ë‹¨í•œ pseudo codeëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 

#### forward 
```python 
# ì»¤ë„ì´ ì‹¤í–‰ë˜ê¸°ì „ì— ë‹¤ìŒê³¼ ê°™ì€ tensorë“¤ì´ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì • 
# in_tesor   (shape = (seq_len, batch, n_head, dim_head))  
# freq       (shape = (seq, 1, 1, dim_head // 2)),        ; ë¯¸ë¦¬ ê³„ì‚°í•´ë‘” (m x theta) array 
# out_tensor (shape = (seq_len, batch, n_head, dim_head)) ; output tensorê°€ ì €ì¥ë  ìœ„ì¹˜ 

a, b, c = select based on tl.program_id()

# load 
in_first_half = LOAD(in_tensor[a, b, c, :dim//2])
in_second_half = LOAD(in_tensor[a, b, c, dim//2:])
freq = LOAD(freq[a, ...])

# compute
first_out = in_first_half * cos(freq) - in_second_half * sin(freq)
second_out = in_second_half * sin(freq) + in_second_half * cos(freq)

#store
STORE(first_out, out_tensor[a, b, c, :dim//2])
STORE(second_out, out_tensor[a, b, c, dim//2:])
```


#### backward
```python 
# ì»¤ë„ì´ ì‹¤í–‰ë˜ê¸°ì „ì— ë‹¤ìŒê³¼ ê°™ì€ tensorë“¤ì´ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
# in_grad_tesor   (shape = (seq_len, batch, n_head, dim_head))  
# freq            (shape = (seq, 1, 1, dim_head // 2)),        ; ë¯¸ë¦¬ ê³„ì‚°í•´ë‘” (m x theta) array 
# out_grad_tensor (shape = (seq_len, batch, n_head, dim_head)) ; output tensorê°€ ì €ì¥ë  ìœ„ì¹˜ 

a, b, c = select based on tl.program_id() 

# load 
in_first_half = LOAD(in_grad_tensor[a, b, c, :dim//2])
in_second_half = LOAD(in_grad_tensor[a, b, c, dim//2:])
freq = LOAD(freq[a, ...])

# compute
out_grad_first = in_first_half * cos(freq) + in_second_half * sin(freq)
out_grad_second  = -in_first_half * sin(freq) + in_second_half * cos(freq)

#store
STORE(out_grad_first, out_grad_tensor[a, b, c, :dim//2])
STORE(out_grad_second, out_grad_tensor[a, b, c, dim//2:])
``` 
