

# Requirement
* docker engine
* nvidia gpu drivers
  * driver version 535.161.07 tested on my local env

# Env setting

* Build a docker image
  ```bash
  git clone https://github.com/zetwhite/triton-practice.git 
  cd triton-practice
  
  docker build -t nvidia_env:1.0 -f nvidia_env.dockerfile .
  ```

* Run a container
  ```bash
  cd triton-practice
  
  docker run \
    -v ./RoPE:/home/RoPE \
    --gpus all \
    --ipc=host \
    --ulimit memlock=-1 \
    --ulimit stack=67108864 \
    --rm -it nvidia_env:1.0
  ```

# Run
* After attaching the container, you could run the code in this directory.
   ```bash
   # inside the container, don't forget to move this directroy 
   cd /home/RoPE
   ```

* `value_check.py` ëŠ” tritonìœ¼ë¡œ ì§ì ‘ êµ¬í˜„í•œ RoPE forward, backword í•¨ìˆ˜ì™€ [Transformer Engineì— ìˆëŠ” í•¨ìˆ˜](https://github.com/NVIDIA/TransformerEngine/blob/5b90b7f5ed67b373bc5f843d1ac3b7a8999df08e/transformer_engine/pytorch/attention.py#L1037-L1078)ì˜ í…ì„œê°’ì„ ë¹„êµí•©ë‹ˆë‹¤. 
    ```bash
    python3 value_check.py

    # wait for a moment
    # expected log
    # Let's check accuracy, by comparing transformer_engine implementation and my own kernel :D
    #
    # >> forward
    #  - RoPE_forward() output is exactly SAME with TransformerEngine's
    #
    # >> backward
    #  - RoPE_backward() output is exactly SAME with TransformerEngine's
    # 
    ``` 

* `perf.py`ëŠ” ì§ì ‘ êµ¬í˜„í•œ RoPE forwardí•¨ìˆ˜ì™€ Transformer Engineì˜ í•¨ìˆ˜ì˜ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    * ë‹¤ë§Œ, ì—¬ê¸°ì„œ ë¹„êµë˜ëŠ” Transformer Engineì˜ RoPEí•¨ìˆ˜(`apply_rotary_pos_embed`)ëŠ” [fused](https://github.com/NVIDIA/TransformerEngine/commit/6c1a8bb5dffbce386380f8e5a12c45f7032d9b76#diff-8215778f23231390f7e41e1339eed64843646d7aba265b8dbf3d68a76c1a647f)ê°€ ì ìš©ë˜ê¸°ì „ì˜ í•¨ìˆ˜ ì…ë‹ˆë‹¤.
    * <sub> ì•„ì‰½ê²Œë„ ë¡œì»¬ì—ì„œ ë©”ëª¨ë¦¬ê°€ ìµœì‹ ë²„ì „ì˜ Transformer Engineì„ ë¹Œë“œí•˜ì§€ ëª» í–ˆìŠµë‹ˆë‹¤. ğŸ˜¢ </sub>  
    ``` bash 
    python perf.py
    
    # this takes some times... 
    # you could check a simple result image(*.png) in perf
    ```

* seq_lenì— ë”°ë¥¸ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 
  * <img src="https://github.com/zetwhite/triton-practice/assets/61981457/bb6b90b2-eb28-4328-a73e-72a32640d6b0" width="30%">


# Note 
forwardì™€ backward ì»¤ë„ì— ëŒ€í•œ pseudo codeëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 

#### formula  

<img src="https://github.com/zetwhite/triton-practice/assets/61981457/9ca4e6ae-7667-4b4c-9ccd-23b741860278" width="40%"> 
<sub> from ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING, https://arxiv.org/pdf/2104.09864.pdf </sub>

#### forward 
```python 
# ì»¤ë„ì´ ì‹¤í–‰ë˜ê¸°ì „ì— ë‹¤ìŒê³¼ ê°™ì€ tensorë“¤ì´ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì • 
# in_tesor   (shape = (seq_len, batch, n_head, dim_head))  
# freq       (shape = (seq, 1, 1, dim_head // 2)),        ; ë¯¸ë¦¬ ê³„ì‚°í•´ë‘” (m x theta) array 
# out_tensor (shape = (seq_len, batch, n_head, dim_head)) ; output tensorê°€ ì €ì¥ë  ìœ„ì¹˜ 

# ìµœëŒ€ a*b*cê°œ ë³‘ë ¬ ìˆ˜í–‰
a, b, c = select based on tl.program_id()

# load 
in_first_half = LOAD(in_tensor[a, b, c, :dim//2])
in_second_half = LOAD(in_tensor[a, b, c, dim//2:])
freq = LOAD(freq[a, 1, 1, :])

# compute
first_out = in_first_half * cos(freq) - in_second_half * sin(freq)
second_out = in_second_half * sin(freq) + in_second_half * cos(freq)

#store
STORE(first_out, out_tensor[a, b, c, :dim//2])
STORE(second_out, out_tensor[a, b, c, dim//2:])
```


#### backward
```python 
# ì»¤ë„ì´ ì‹¤í–‰ë˜ê¸°ì „ì— ë‹¤ìŒê³¼ ê°™ì€ tensorë“¤ì´ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
# in_grad_tesor   (shape = (seq_len, batch, n_head, dim_head))  
# freq            (shape = (seq, 1, 1, dim_head // 2)),        ; ë¯¸ë¦¬ ê³„ì‚°í•´ë‘” (m x theta) array 
# out_grad_tensor (shape = (seq_len, batch, n_head, dim_head)) ; output tensorê°€ ì €ì¥ë  ìœ„ì¹˜ 

# ìµœëŒ€ a*b*cê°œ ë³‘ë ¬ ìˆ˜í–‰ 
a, b, c = select based on tl.program_id() 

# load 
in_first_half = LOAD(in_grad_tensor[a, b, c, :dim//2])
in_second_half = LOAD(in_grad_tensor[a, b, c, dim//2:])
freq = LOAD(freq[a, 1, 1, :])

# compute
out_grad_first = in_first_half * cos(freq) + in_second_half * sin(freq)
out_grad_second  = -in_first_half * sin(freq) + in_second_half * cos(freq)

#store
STORE(out_grad_first, out_grad_tensor[a, b, c, :dim//2])
STORE(out_grad_second, out_grad_tensor[a, b, c, dim//2:])
``` 
