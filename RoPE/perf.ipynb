{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RoPE_transformer_engine' from '/home/triton-practice/RoPE/RoPE_transformer_engine.py'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "\n",
    "import RoPE_transformer_engine as base\n",
    "import RoPE_forward as my_forward\n",
    "\n",
    "import importlib \n",
    "importlib.reload(base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seq_len'],\n",
    "        x_vals=[2**i for i in range(5, 20, 1)],\n",
    "        x_log=True,                                 # logarithmic in x axis\n",
    "        line_arg = 'provider', \n",
    "        line_vals = ['my', 'base'],\n",
    "        line_names = ['my', 'transformer_engine'],\n",
    "        styles=[('blue', '-'), ('green','-')],\n",
    "        ylabel = 'GB/s',\n",
    "        plot_name = 'RoPE forward() perf',\n",
    "        args = {'batch':1, 'n_head':8, 'dim_head':64}, \n",
    "    )) \n",
    "def benchmark(seq_len, batch, n_head, dim_head, provider) :\n",
    "    \n",
    "    # prepare input\n",
    "    size = (seq_len, batch, n_head, dim_head)\n",
    "    input = torch.rand(size, device='cuda:0', dtype=torch.float32)\n",
    "   \n",
    "    # prepare base \n",
    "    max_seq = 2 ** 20 \n",
    "    te_base = base.TransformerEngineRoPE(max_seq = max_seq, hidden_size=dim_head)\n",
    "    \n",
    "    # prepare my\n",
    "    freq =my_forward.create_freq(max_seq, dim_head).to('cuda:0')\n",
    "    \n",
    "    quantiles = [0.2, 0.5, 0.8]\n",
    "    if provider == 'base':\n",
    "        min_ms, ms, max_ms = triton.testing.do_bench(lambda: te_base.forward(input), quantiles=quantiles)\n",
    "    if provider == 'my':\n",
    "        min_ms, ms, max_ms = triton.testing.do_bench(lambda: my_forward.RoPE_fwd(input, freq), quantiles=quantiles)\n",
    "    \n",
    "    gbps = lambda ms : torch.numel(input) * input.element_size() / ms * 1e-6\n",
    "    \n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
